{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c05577-ef98-482a-a275-165da49ee85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec46d4f-162c-4da6-960b-844036879f05",
   "metadata": {},
   "source": [
    "Define the loss function of AFT model with log normal distribution assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88050288-8117-4002-996c-a4f18cf35193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aft_ln_loss(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    labels = np.abs(y_true)\n",
    "    nsamp = len(y_true)\n",
    "    yy = np.log(np.abs(y_true)) - preds\n",
    "    indicator_event = np.array(y_true > 0)\n",
    "    indicator_censor = np.array(y_true < 0)\n",
    "    censor = yy[indicator_censor]\n",
    "    dcensor = norm.pdf(censor)\n",
    "    pcensor = norm.cdf(-censor)\n",
    "    grad = np.ones(nsamp)\n",
    "    hess = np.ones(nsamp)\n",
    "    grad[indicator_event] = -yy[indicator_event]\n",
    "    grad[indicator_censor] = -dcensor / pcensor\n",
    "    hess[indicator_censor] = dcensor * (dcensor - censor * pcensor) / (pcensor ** 2)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61b9fd-9c52-49b0-bb65-e26087725450",
   "metadata": {},
   "source": [
    "Define the loss function of AFT model with exponential distribution assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f334ccce-cb2a-4cbd-9b48-46dd0c934494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aft_exp_loss(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    ey = np.exp(np.log(np.abs(y_true)) - preds)\n",
    "    nsamp = len(y_true)\n",
    "    indicator_event = np.array(y_true > 0)\n",
    "    indicator_censor = np.array(y_true < 0)\n",
    "    event = ey[indicator_event]\n",
    "    censor = ey[indicator_censor]\n",
    "    grad = np.ones(nsamp)\n",
    "    hess = np.ones(nsamp)\n",
    "    grad[indicator_event] = (1 - event) / (1 + event)\n",
    "    grad[indicator_censor] = -1 / (1 + 1 / censor)\n",
    "    hess[indicator_event] = 2 * event / ((1 + event) ** 2)\n",
    "    hess[indicator_censor] = (1 + censor) ** (-2)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29185651-6955-4358-b6ba-6c4e65890ca8",
   "metadata": {},
   "source": [
    "Define the loss function of AFT model with Weibull distribution assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e356d79-9943-4b13-9a0e-b6560548cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aft_weibull_loss(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    ey = np.exp(np.log(np.abs(y_true)) - preds)\n",
    "    indicator_event = np.array(y_true > 0)\n",
    "    grad = -ey\n",
    "    grad[indicator_event] = 1 - ey[indicator_event]\n",
    "    return grad, ey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772a8f7-4344-43aa-98fd-32ee25876d6c",
   "metadata": {},
   "source": [
    "Define the evaluation metric with Concordance index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9e357a3-1be7-4081-84b8-a97d5a97b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalc_aft(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    return 'concordance_index', concordance_index_censored(np.array(y_true > 0), abs(y_true), -preds)[0], True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2a7dd-0e02-4657-aacc-2f1f90efa3ea",
   "metadata": {},
   "source": [
    "load sample training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "571d3b27-2fed-4865-9293-df5e1bcd164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr = pd.read_csv('xtr.csv')\n",
    "xva = pd.read_csv('xva.csv')\n",
    "xte = pd.read_csv('xte.csv')\n",
    "ytr = pd.read_csv('ytr.csv')\n",
    "yva = pd.read_csv('yva.csv')\n",
    "yte = pd.read_csv('yte.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723314c-6e50-43eb-b964-fe6814914814",
   "metadata": {},
   "source": [
    "Parameters can be customized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe586655-0eb7-41a4-b4a0-210ce72b2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c9ef0-aced-4370-a185-b5f0e8f7016a",
   "metadata": {},
   "source": [
    "Transfer training and validation data from data frame to LGB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea0f4ffd-a915-435e-be83-b94969aa74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(xtr, ytr)\n",
    "lgb_eval = lgb.Dataset(xva, yva, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab93780-75ab-43c8-b0ff-537316b0596e",
   "metadata": {},
   "source": [
    "Train LGB AFT model with log normal, exponential and Weibull distributions. Using validation data for early stop. the number of interation and early stop steps can be customized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1ab1f75-4c2e-4258-9ca8-96010a62b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's concordance_index: 0.829046\n"
     ]
    }
   ],
   "source": [
    "model_aftln = lgb.train(params, train_set = lgb_train, valid_sets = lgb_eval, num_boost_round = 1000, callbacks=[lgb.early_stopping(20)] ,fobj = aft_ln_loss, feval = evalc_aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16f79a50-7323-4863-a3cb-80bfd5fcd220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's concordance_index: 0.77013\n"
     ]
    }
   ],
   "source": [
    "model_aftexp = lgb.train(params, train_set = lgb_train, valid_sets = lgb_eval, num_boost_round = 1000, callbacks=[lgb.early_stopping(20)] ,fobj = aft_exp_loss, feval = evalc_aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "348ddb1a-3c5e-4421-b358-8e453dbed659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's concordance_index: 0.746645\n"
     ]
    }
   ],
   "source": [
    "model_aftwei = lgb.train(params, train_set = lgb_train, valid_sets = lgb_eval, num_boost_round = 1000, callbacks=[lgb.early_stopping(20)] ,fobj = aft_weibull_loss, feval = evalc_aft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a20ca-1071-423f-b50f-1d60b3a4af2f",
   "metadata": {},
   "source": [
    "Make prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a02c939-7920-43eb-a9ab-36f25575f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyaftln = -model_aftln.predict(xte)\n",
    "yyaftexp = -model_aftexp.predict(xte)\n",
    "yyaftwei = -model_aftwei.predict(xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29905616-f7f6-4e3a-9207-5a3c339219dd",
   "metadata": {},
   "source": [
    "Compute concordance index and select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "101d73fe-295c-4a17-973c-d4a5f32a69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidxlist = {'log_normal': concordance_index_censored(np.array(yte > 0).reshape(200,), np.array(abs(yte)).reshape(200,), yyaftln)[0],\n",
    "           'exponential': concordance_index_censored(np.array(yte > 0).reshape(200,), np.array(abs(yte)).reshape(200,), yyaftexp)[0],\n",
    "           'weibull': concordance_index_censored(np.array(yte > 0).reshape(200,), np.array(abs(yte)).reshape(200,), yyaftwei)[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcc3c682-4b56-48f3-bd7a-6dec248b9cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_normal': 0.8160516129032258,\n",
       " 'exponential': 0.7418838709677419,\n",
       " 'weibull': 0.7301161290322581}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cidxlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4ab99-2469-4077-a486-919c50dcf9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
